{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMI9sK9wzI8n"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Input, Add, Conv2D, Reshape, GlobalAveragePooling1D, Dropout, Flatten\n",
    "from tensorflow.keras.activations import gelu\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.datasets import mnist, cifar100\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vit_keras import vit, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NpsTIteYzLTi"
   },
   "outputs": [],
   "source": [
    "from helper_functions import *\n",
    "from mlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup file directory\n",
    "file_path = \"/home/ecbm4040/e4040-2023fall-project-mlpm-hb2776-dg3370-amp2365\"\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "import os\n",
    "os.chdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWg3hvqFzOfi"
   },
   "outputs": [],
   "source": [
    "# check availability of GPU\n",
    "print(tf.__version__)\n",
    "\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU is available.\")\n",
    "    print(\"Available GPUs:\")\n",
    "    for gpu in tf.config.list_physical_devices('GPU'):\n",
    "        print(gpu)\n",
    "else:\n",
    "    print(\"CPU is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-dj6xMwzVmB"
   },
   "source": [
    "# Load the Imagenet 1000 (mini) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-l6d3xMzWRi"
   },
   "outputs": [],
   "source": [
    "# Define a function to load a dataset from folders using ImageDataGenerator\n",
    "def load_dataset_from_folders(main_folder, image_size=(224, 224), batch_size=16):\n",
    "    # Create an ImageDataGenerator with rescaling\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # Generate a flow of data from the specified directory\n",
    "    dataset = datagen.flow_from_directory(\n",
    "        main_folder,\n",
    "        target_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='sparse'\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Define the path to the training folder and load the training dataset\n",
    "train_folder_path = os.path.join(file_path, \"imagenet-mini\", \"train\")\n",
    "train_dataset = load_dataset_from_folders(train_folder_path)\n",
    "\n",
    "# Display information about the training dataset\n",
    "print(\"Number of batches:\", len(train_dataset))\n",
    "print(\"Batch shape:\", train_dataset[0][0].shape)\n",
    "\n",
    "# Define the path to the validation folder and load the validation dataset\n",
    "val_folder_path = os.path.join(file_path, \"imagenet-mini\", \"val\")\n",
    "val_dataset = load_dataset_from_folders(val_folder_path)\n",
    "\n",
    "# Display information about the validation dataset\n",
    "print(\"Number of batches:\", len(val_dataset))\n",
    "print(\"Batch shape:\", val_dataset[0][0].shape)\n",
    "\n",
    "# Define the input shape and number of classes for the ImageNet dataset\n",
    "image_net_input_shape = (224, 224, 3)\n",
    "image_net_num_classes = 945  # Note: Please check this value again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5J0X0BXkInk"
   },
   "source": [
    "# pretrain MLP mixer on Imagenet (1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFxSsdEUzSnl"
   },
   "outputs": [],
   "source": [
    "# Function for gradient clipping\n",
    "def clip_norm(gradients, clip_value):\n",
    "    # Clip gradients to a specified range\n",
    "    return K.clip(gradients, -clip_value, clip_value)\n",
    "\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "class CosineAnnealingScheduler(Callback):\n",
    "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
    "        # Cosine annealing learning rate scheduler\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Callback at the beginning of each epoch\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        lr = self.eta_min + 0.5 * (self.eta_max - self.eta_min) * (1 + np.cos(np.pi * epoch / self.T_max))\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "class WarmUpLearningRateScheduler(Callback):\n",
    "    def __init__(self, warmup_batches, init_lr, verbose=0):\n",
    "        # Warm-up learning rate scheduler\n",
    "        super(WarmUpLearningRateScheduler, self).__init__()\n",
    "        self.warmup_batches = warmup_batches\n",
    "        self.init_lr = init_lr\n",
    "        self.verbose = verbose\n",
    "        self.current_batch = 0\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # Callback at the beginning of each batch\n",
    "        if self.current_batch <= self.warmup_batches:\n",
    "            lr = self.current_batch * self.init_lr / self.warmup_batches\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "            if self.verbose > 0:\n",
    "                print('\\nBatch %05d: WarmUpLearningRateScheduler setting learning rate to %s.' % (self.current_batch + 1, lr))\n",
    "        self.current_batch += 1\n",
    "\n",
    "# Set hyperparameters\n",
    "warmup_batches = 500\n",
    "init_lr = 0.001\n",
    "\n",
    "# Create the callbacks for cosine annealing and warm-up\n",
    "cosine_annealing = CosineAnnealingScheduler(T_max=100, eta_max=0.001, eta_min=0.0001, verbose=1)\n",
    "warmup_lr = WarmUpLearningRateScheduler(warmup_batches=warmup_batches, init_lr=init_lr, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDjaIYx8kBWj"
   },
   "outputs": [],
   "source": [
    "# Define hyperparameters and model architecture for MLP model\n",
    "epsilon = 1e-3\n",
    "input_shape = image_net_input_shape\n",
    "number_of_mixers = 24\n",
    "token_mixing_num_mlps = 512\n",
    "channel_mixing_num_mlps = 4096\n",
    "patch_size = 16\n",
    "hidden_dims = 1024\n",
    "num_classes = image_net_num_classes\n",
    "\n",
    "# Display separator for clarity\n",
    "print(\" \")\n",
    "print(\"+\" * 50)\n",
    "print(\" \")\n",
    "\n",
    "# Create MLP model using specified parameters\n",
    "mlp_model = makeModel(\n",
    "    input_shape=input_shape,\n",
    "    number_of_mixers=number_of_mixers,\n",
    "    token_mixing_num_mlps=token_mixing_num_mlps,\n",
    "    channel_mixing_num_mlps=channel_mixing_num_mlps,\n",
    "    patch_size=patch_size,\n",
    "    hidden_dims=hidden_dims,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "# Compile the model using the Adam optimizer and specified learning rate\n",
    "clip_value = 1.0\n",
    "mlp_model.compile(optimizer=Adam(learning_rate=0.001, clipnorm=clip_value),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Display model summary\n",
    "mlp_model.summary()\n",
    "\n",
    "# Train the MLP model using the specified datasets and callbacks\n",
    "mlp_history = mlp_model.fit(train_dataset, epochs=30, validation_data=val_dataset, callbacks=[cosine_annealing, warmup_lr])\n",
    "\n",
    "# Save the trained model and its history\n",
    "save_data(file_path, mlp_model, mlp_history, \"mlp_imnet_mini\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
