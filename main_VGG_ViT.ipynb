{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXnGmSlxk1V3"
   },
   "source": [
    "[Imagenet (mini) 1000 dataset](https://www.kaggle.com/datasets/ifigotin/imagenetmini-1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mh-UVXikZTZw"
   },
   "source": [
    "[main paper](https://arxiv.org/pdf/2105.01601.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxranTkVmPpn",
    "outputId": "b58e45d4-5b27-44da-9a70-c60f7f84198c"
   },
   "outputs": [],
   "source": [
    "# download all required dependencies\n",
    "! pip install datasets\n",
    "! pip install vit-keras\n",
    "! pip install tensorflow-addons\n",
    "! pip install tensorflow tensorflow-hub\n",
    "! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MGY_MkP0Lc40",
    "outputId": "73f9fea1-50a6-4aac-c000-2abab7bf6e3d"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Input, Add, Conv2D, Reshape, GlobalAveragePooling1D, Dropout, Flatten\n",
    "from tensorflow.keras.activations import gelu\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.datasets import mnist, cifar100\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vit_keras import vit, utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uz1JkBccTgl6",
    "outputId": "17f0f221-5997-43ad-ce17-595725f3caf6"
   },
   "outputs": [],
   "source": [
    "# setup file directory\n",
    "file_path = \"/home/ecbm4040/e4040-2023fall-project-mlpm-hb2776-dg3370-amp2365\"\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "import os\n",
    "os.chdir(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOyjNA85hjPv"
   },
   "outputs": [],
   "source": [
    "from helper_functions import *\n",
    "from mlp import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pVPVTHi3Yrzt",
    "outputId": "d466051e-78e2-4ede-dab8-d887678c3ad8"
   },
   "outputs": [],
   "source": [
    "# check availability of GPU\n",
    "print(tf.__version__)\n",
    "\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU is available.\")\n",
    "    print(\"Available GPUs:\")\n",
    "    for gpu in tf.config.list_physical_devices('GPU'):\n",
    "        print(gpu)\n",
    "else:\n",
    "    print(\"CPU is available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCv61nmbZ1-t"
   },
   "source": [
    "# Fine tuning metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYdt8VcvHQja"
   },
   "outputs": [],
   "source": [
    "# Function for gradient clipping\n",
    "def clip_norm(gradients, clip_value):\n",
    "    # Clip gradients to a specified range\n",
    "    return K.clip(gradients, -clip_value, clip_value)\n",
    "\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "class CosineAnnealingScheduler(Callback):\n",
    "    def __init__(self, T_max, eta_max, eta_min=0, verbose=0):\n",
    "        # Cosine annealing learning rate scheduler\n",
    "        super(CosineAnnealingScheduler, self).__init__()\n",
    "        self.T_max = T_max\n",
    "        self.eta_max = eta_max\n",
    "        self.eta_min = eta_min\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Callback at the beginning of each epoch\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        lr = self.eta_min + 0.5 * (self.eta_max - self.eta_min) * (1 + np.cos(np.pi * epoch / self.T_max))\n",
    "        K.set_value(self.model.optimizer.lr, lr)\n",
    "        if self.verbose > 0:\n",
    "            print('\\nEpoch %05d: CosineAnnealingScheduler setting learning rate to %s.' % (epoch + 1, lr))\n",
    "\n",
    "class WarmUpLearningRateScheduler(Callback):\n",
    "    def __init__(self, warmup_batches, init_lr, verbose=0):\n",
    "        # Warm-up learning rate scheduler\n",
    "        super(WarmUpLearningRateScheduler, self).__init__()\n",
    "        self.warmup_batches = warmup_batches\n",
    "        self.init_lr = init_lr\n",
    "        self.verbose = verbose\n",
    "        self.current_batch = 0\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # Callback at the beginning of each batch\n",
    "        if self.current_batch <= self.warmup_batches:\n",
    "            lr = self.current_batch * self.init_lr / self.warmup_batches\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "            if self.verbose > 0:\n",
    "                print('\\nBatch %05d: WarmUpLearningRateScheduler setting learning rate to %s.' % (self.current_batch + 1, lr))\n",
    "        self.current_batch += 1\n",
    "\n",
    "# Set hyperparameters\n",
    "warmup_batches = 500\n",
    "init_lr = 0.001\n",
    "\n",
    "# Create the callbacks for cosine annealing and warm-up\n",
    "cosine_annealing = CosineAnnealingScheduler(T_max=100, eta_max=0.001, eta_min=0.0001, verbose=1)\n",
    "warmup_lr = WarmUpLearningRateScheduler(warmup_batches=warmup_batches, init_lr=init_lr, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpRsu-Q-lrq8"
   },
   "source": [
    "# Load the tiny-ImageNet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j9hakBu-j16r"
   },
   "outputs": [],
   "source": [
    "def plot_tiny_imnet_data(data, rows=10, cols=10):\n",
    "    # Generate random indices to select images from the dataset\n",
    "    random_indices = tf.random.uniform(shape=(rows*cols,), maxval=len(data), dtype=tf.int32)\n",
    "    \n",
    "    # Create subplots for displaying images\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(20, 20))\n",
    "    fig.suptitle(\"16 random Images from the dataset\", fontsize=16)\n",
    "    \n",
    "    # Loop through subplots and display images\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "      # Extract image data and convert to numpy array\n",
    "      image = np.array((data[int(random_indices[i])][\"image\"]).convert('RGB'))\n",
    "      ax.imshow(image)\n",
    "      ax.set_title(\"{}\".format(data[random_indices[i]][\"label\"]))\n",
    "      ax.axis('off')\n",
    "    \n",
    "    # Adjust subplot spacing\n",
    "    plt.subplots_adjust(wspace=0.7, hspace=0.1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def tiny_imagenet_data_generator(data, batch_size=32, shuffle=True):\n",
    "  # Define the target image size\n",
    "  target_size = (224, 224, 3)\n",
    "  indices = np.arange(len(data))\n",
    "    \n",
    "  # Create an array of indices for the dataset\n",
    "  while True:\n",
    "    # Shuffle the indices if required\n",
    "    if shuffle:\n",
    "      np.random.shuffle(indices)\n",
    "\n",
    "    for i in range(0, len(indices), batch_size):\n",
    "      # Extract batch indices and corresponding data\n",
    "      batch_indices = np.array(indices[i:i+batch_size]).astype('int16')\n",
    "      batch_data = [data[int(idx)] for idx in batch_indices]\n",
    "      # Resize and preprocess images\n",
    "      batch_images = [np.array(image['image'].convert('RGB').resize(target_size[:2])) for image in batch_data]\n",
    "      batch_labels = [image['label'] for image in batch_data]\n",
    "      \n",
    "      # Extract and convert batch labels      \n",
    "      batch_images = np.stack(batch_images)\n",
    "      batch_images = batch_images.astype('float32') / 255.0\n",
    "      batch_labels = np.array(batch_labels).astype('int16')\n",
    "      \n",
    "      yield batch_images, batch_labels\n",
    "\n",
    "#  Define the number of classes and shape of images\n",
    "tinyimagenet_num_classes = 200\n",
    "tinyimagenet_shape = (64, 64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SEO1huvhuoiF"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Load the Tiny Imagenet dataset\n",
    "tiny_imagenet_train = load_dataset('Maysee/tiny-imagenet', split='train')\n",
    "tiny_imagenet_valid = load_dataset('Maysee/tiny-imagenet', split='valid')\n",
    "\n",
    "# Calculate and print the time taken to download and split the Tiny Imagenet data\n",
    "print(\"Downloading and splitting the tiny Imagenet data takes {} seconds\".format(round(time.time() - start_time, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rH7FdZT7eG_X"
   },
   "source": [
    "# Load the CIFAR-100 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ePmRsSEcepiS",
    "outputId": "f9fb15b4-cd22-40df-e4ed-61e5fd9ec0d0"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR-100 dataset\n",
    "(cifar_x_train, cifar_y_train), (cifar_x_test, cifar_y_test) = cifar100.load_data()\n",
    "\n",
    "# Check if the input images are grayscale or color\n",
    "if len(cifar_x_train.shape) == 3:\n",
    "  cifar_input_shape = (cifar_x_train.shape[1], cifar_x_train.shape[2], 1)\n",
    "elif len(cifar_x_train.shape) == 4:\n",
    "  cifar_input_shape = cifar_x_train.shape[1:]\n",
    "\n",
    "# Store input, output, and class information\n",
    "cifar_train_input_shape = cifar_x_train.shape\n",
    "cifar_test_input_shape = cifar_x_test.shape\n",
    "cifar_output_shape = cifar_y_train.shape\n",
    "cifar_output_classes = np.unique(cifar_y_train)\n",
    "cifar_num_classes = len(cifar_output_classes)\n",
    "\n",
    "# Print dataset information\n",
    "print(\"Shape of training data:\", cifar_train_input_shape)\n",
    "print(\"Shape of testing data:\", cifar_test_input_shape)\n",
    "print(\"Number of Output classes:\", cifar_num_classes)\n",
    "print(\"Output classes:\", cifar_output_classes)\n",
    "\n",
    "# Define a CIFAR data generator function\n",
    "def cifar_data_generator(data, labels, batch_size=32, shuffle=True):\n",
    "    # Define the target image size for resizing\n",
    "    target_size = (224, 224, 3)\n",
    "    \n",
    "    # Create an array of indices for the dataset\n",
    "    indices = np.arange(len(data))\n",
    "\n",
    "    while True:\n",
    "      if shuffle:\n",
    "        # Shuffle the indices if required\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "      for i in range(0, len(indices), batch_size):\n",
    "        # Extract batch indices and resize images\n",
    "        batch_images = tf.image.resize(data[i: i + batch_size, ] / 255.0, target_size[:2]).numpy()\n",
    "        batch_labels = labels[i: i + batch_size]\n",
    "\n",
    "        yield batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmKLApqhfwl9"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHWOMHsW1Fis"
   },
   "source": [
    "# VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJgjRhKsr8aE"
   },
   "outputs": [],
   "source": [
    "def create_vgg(output_class):\n",
    "  # Load the pre-trained VGG16 model with ImageNet weights\n",
    "  base_model = VGG16(\n",
    "      include_top=True,\n",
    "      weights=\"imagenet\",\n",
    "      input_tensor=None,\n",
    "      input_shape=None,\n",
    "      pooling=None,\n",
    "      classes=1000,\n",
    "      classifier_activation=\"softmax\",\n",
    "  )\n",
    "  \n",
    "  # Freeze the layers of the base model\n",
    "  for layer in base_model.layers:\n",
    "      layer.trainable = False\n",
    "  \n",
    "  # Create a custom top layer for the model\n",
    "  x = (base_model.output)\n",
    "  x = Flatten()(x)\n",
    "  x = Dense(512, activation='relu')(x)\n",
    "  x = Dense(256, activation='relu')(x)\n",
    "    \n",
    "  # Output layer with the specified number of classes and softmax activation  \n",
    "  output = Dense(output_class, activation='softmax')(x)\n",
    "  \n",
    "  # Create the final model with the base model as the input and custom top layer as output\n",
    "  return Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwzesqr5YA5K"
   },
   "source": [
    "# VGG16 on tiny image net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aXYptM3dYAHX",
    "outputId": "c974a93e-9366-400a-d0b3-4d89fa41a9fd"
   },
   "outputs": [],
   "source": [
    "# Set the clipping value for gradient clipping\n",
    "clip_value = 1.0\n",
    "\n",
    "# Create a VGG16-based model for Tiny Imagenet with the specified number of classes\n",
    "vgg16_tiny_imnet_model = create_vgg(tinyimagenet_num_classes)\n",
    "\n",
    "# Compile the model with Adam optimizer and sparse categorical crossentropy loss\n",
    "vgg16_tiny_imnet_model.compile(optimizer=Adam(learning_rate=0.001, clipnorm=clip_value), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "vgg16_tiny_imnet_model.summary()\n",
    "\n",
    "# Create data generators for training and validation from Tiny Imagenet datasets\n",
    "vgg_tinyimnet_train_data = tiny_imagenet_data_generator(tiny_imagenet_train)\n",
    "vgg_tinyimnet_val_data = tiny_imagenet_data_generator(tiny_imagenet_valid)\n",
    "\n",
    "# Train the model with specified callbacks and evaluate on training and validation sets\n",
    "start_time = time.time()\n",
    "vgg_tiny_imnet_history = vgg16_tiny_imnet_model.fit(vgg_tinyimnet_train_data,\n",
    "                                                    epochs=10,\n",
    "                                                    steps_per_epoch=len(tiny_imagenet_train) // 32 + 1,\n",
    "                                                    validation_data=vgg_tinyimnet_val_data,\n",
    "                                                    validation_steps=len(tiny_imagenet_valid) // 32 + 1,\n",
    "                                                    callbacks=[cosine_annealing, warmup_lr])\n",
    "vgg_tiny_imnet_time = time.time() - start_time\n",
    "print(\"Time to fine-tune VGG16 on Tiny Imagenet dataset: {}\".format(round(vgg_tiny_imnet_time, 4)))\n",
    "\n",
    "# Evaluate the model on training and validation datasets\n",
    "vgg_tinyimnet_train_result = vgg16_tiny_imnet_model.evaluate(vgg_tinyimnet_train_data, steps=len(tiny_imagenet_train) // 32 + 1)\n",
    "vgg_tinyimnet_val_result = vgg16_tiny_imnet_model.evaluate(vgg_tinyimnet_val_data, steps=len(tiny_imagenet_valid) // 32 + 1)\n",
    "\n",
    "# Display training and validation accuracies and losses\n",
    "print('')\n",
    "print('Training Accuracy: {}% and Training Loss: {}'.format(round(vgg_tinyimnet_train_result[1] * 100, 2), round(vgg_tinyimnet_train_result[0] * 100, 2)))\n",
    "print('Testing Accuracy: {}% and Testing Loss: {}'.format(round(vgg_tinyimnet_val_result[1] * 100, 2), round(vgg_tinyimnet_val_result[0]  * 100, 2)))\n",
    "print('')\n",
    "\n",
    "# Save the model and training history\n",
    "save_data(file_path, vgg16_tiny_imnet_model, vgg_tiny_imnet_history, \"vgg16_tiny_imnet\")\n",
    "\n",
    "# Delete variables to free up memory\n",
    "del vgg16_tiny_imnet_model, vgg_tiny_imnet_history, vgg_tinyimnet_train_result, vgg_tinyimnet_val_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SkNU6N4n2sF"
   },
   "source": [
    "# VGG 16 on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TR87u9FHn55h",
    "outputId": "1efdfa82-6d14-4c79-eb31-53bf435604f4"
   },
   "outputs": [],
   "source": [
    "# Set the clipping value and batch size\n",
    "clip_value = 1.0\n",
    "batch_size = 32\n",
    "\n",
    "# Create VGG16 model for CIFAR dataset\n",
    "vgg16_cifar_model = create_vgg(cifar_num_classes)\n",
    "vgg16_cifar_model.compile(optimizer=Adam(learning_rate=0.001, clipnorm=clip_value), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "vgg16_cifar_model.summary()\n",
    "\n",
    "# Create data generators for training and validation sets\n",
    "vgg_cifar_train_data = cifar_data_generator(cifar_x_train, cifar_y_train, batch_size=batch_size, shuffle=True)\n",
    "vgg_cifar_val_data = cifar_data_generator(cifar_x_test, cifar_y_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train the VGG16 model on CIFAR dataset\n",
    "start_time = time.time()\n",
    "vgg_cifar_history = vgg16_cifar_model.fit(vgg_cifar_train_data,\n",
    "                                          epochs=10,\n",
    "                                          steps_per_epoch=len(cifar_x_train) // 32 + 1,\n",
    "                                          validation_data=vgg_cifar_val_data,\n",
    "                                          validation_steps=len(cifar_x_test) // 32 + 1,\n",
    "                                          callbacks=[cosine_annealing, warmup_lr])\n",
    "\n",
    "# Calculate and print the time taken for training\n",
    "vgg_cifar_time = time.time() - start_time\n",
    "print(\"Time to fine-tune VGG16 on CIFAR dataset: {}\".format(round(vgg_cifar_time, 4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the VGG16 model on training and validation data\n",
    "vgg_cifar_train_result = vgg16_cifar_model.evaluate(vgg_cifar_train_data, steps=len(vgg_cifar_train_data) // 32 + 1)\n",
    "vgg_cifar_val_result = vgg16_cifar_model.evaluate(vgg_cifar_val_data, steps=len(vgg_cifar_val_data) // 32 + 1)\n",
    "\n",
    "# Print the evaluation results\n",
    "print('')\n",
    "print('Training Accuracy: {}% and Training Loss: {}'.format(round(vgg_cifar_train_result[1] * 100, 2), round(vgg_cifar_train_result[0] * 100, 2)))\n",
    "print('Testing Accuracy: {}% and Testing Loss: {}'.format(round(vgg_cifar_val_result[1] * 100, 2), round(vgg_cifar_val_result[0]  * 100, 2)))\n",
    "print('')\n",
    "\n",
    "# Save the model data\n",
    "save_data(file_path, vgg16_cifar_model, vgg_cifar_history, \"vgg16_cifar_imnet\")\n",
    "\n",
    "# Delete variables to free up memory\n",
    "del vgg16_cifar_model, vgg_cifar_history, vgg_cifar_train_result, vgg_cifar_val_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFM9JIIk4qLz"
   },
   "source": [
    "# ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5-PtJBA4jUY"
   },
   "outputs": [],
   "source": [
    "# Function to create a Vision Transformer (ViT) model with a specified output class\n",
    "def create_vit(output_class):\n",
    "  # Load ViT-B16 model with specific configurations\n",
    "  base_model = vit.vit_b16(\n",
    "      image_size=224,\n",
    "      activation='sigmoid',\n",
    "      pretrained=True,\n",
    "      include_top=False,\n",
    "      pretrained_top=False,\n",
    "  )\n",
    "\n",
    "  # Freeze the layers of the base model\n",
    "  for layer in base_model.layers:\n",
    "      layer.trainable = False\n",
    "\n",
    "  # Additional layers for classification\n",
    "  x = (base_model.output)\n",
    "  x = Flatten()(x)\n",
    "  x = Dense(256, activation='relu')(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "  output = Dense(output_class, activation='softmax', kernel_regularizer=l2(0.002))(x)\n",
    "\n",
    "  # Create and return the complete model\n",
    "  return Model(inputs=base_model.input, outputs=output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7SHcjOsz1Cv"
   },
   "source": [
    "# ViT on tiny-ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Fe2hvSI_z3Wt",
    "outputId": "b5414037-f156-4f80-b392-afaffbe1b164"
   },
   "outputs": [],
   "source": [
    "# Set the value for gradient clipping\n",
    "clip_value = 1.0\n",
    "\n",
    "# Create a Vision Transformer (ViT) model for Tiny Imagenet classification\n",
    "vit_tiny_imnet_model = create_vit(tinyimagenet_num_classes)\n",
    "\n",
    "# Compile the ViT model with Adam optimizer and specified gradient clipping\n",
    "vit_tiny_imnet_model.compile(optimizer=Adam(learning_rate=0.001, clipnorm=clip_value), \n",
    "                             loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the summary of the ViT model\n",
    "vit_tiny_imnet_model.summary()\n",
    "\n",
    "# Generate data using the Tiny Imagenet data generator\n",
    "vit_tinyimnet_train_data = tiny_imagenet_data_generator(tiny_imagenet_train)\n",
    "vit_tinyimnet_val_data = tiny_imagenet_data_generator(tiny_imagenet_valid)\n",
    "\n",
    "# Train the ViT model on the Tiny Imagenet dataset\n",
    "start_time = time.time()\n",
    "vit_tiny_imnet_history = vit_tiny_imnet_model.fit(vit_tinyimnet_train_data,\n",
    "                                                  epochs=10,\n",
    "                                                  steps_per_epoch=len(tiny_imagenet_train) // 32 + 1,\n",
    "                                                  validation_data=vit_tinyimnet_val_data,\n",
    "                                                  validation_steps=len(tiny_imagenet_valid) // 32 + 1,\n",
    "                                                  callbacks=[cosine_annealing, warmup_lr])\n",
    "\n",
    "# Calculate the time taken for training\n",
    "vit_tinyimnet_time = time.time() - start_time\n",
    "print(\"Time to fine-tune ViT on Tiny Imagenet dataset: {}\".format(round(vit_tinyimnet_time, 4)))\n",
    "\n",
    "# Evaluate the performance on training and validation sets\n",
    "vit_tinyimnet_train_result = vit_tiny_imnet_model.evaluate(vit_tinyimnet_train_data, steps=len(tiny_imagenet_train) // 32 + 1)\n",
    "vit_tinyimnet_val_result = vit_tiny_imnet_model.evaluate(vit_tinyimnet_val_data, steps=len(tiny_imagenet_valid) // 32 + 1)\n",
    "\n",
    "# Display the training and validation metrics\n",
    "print('')\n",
    "print('Training Accuracy: {}% and Training Loss: {}'.format(round(vit_tinyimnet_train_result[1] * 100, 2), round(vit_tinyimnet_train_result[0] * 100, 2)))\n",
    "print('Testing Accuracy: {}% and Testing Loss: {}'.format(round(vit_tinyimnet_val_result[1] * 100, 2), round(vit_tinyimnet_val_result[0]  * 100, 2)))\n",
    "print('')\n",
    "\n",
    "# Save the model data\n",
    "save_data(file_path, vit_tiny_imnet_model, vit_tiny_imnet_history, \"vit_tiny_imnet\")\n",
    "\n",
    "# Clear variables from memory\n",
    "del vit_tiny_imnet_model, vit_tiny_imnet_history, vit_tinyimnet_train_result, vit_tinyimnet_val_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJj3CVE5z4Fv"
   },
   "source": [
    "# ViT on CIFAR-100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuCqCWTyz5-k",
    "outputId": "ef776593-0e3f-4067-ce1b-0aec7c818f91"
   },
   "outputs": [],
   "source": [
    "# Set the value for gradient clipping\n",
    "clip_value = 1.0\n",
    "\n",
    "# Create a Vision Transformer (ViT) model for CIFAR classification\n",
    "vit_cifar_model = create_vit(cifar_num_classes)\n",
    "\n",
    "# Compile the ViT model with Adam optimizer and specified gradient clipping\n",
    "vit_cifar_model.compile(optimizer=Adam(learning_rate=0.001, clipnorm=clip_value), \n",
    "                        loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the summary of the ViT model\n",
    "vit_cifar_model.summary()\n",
    "\n",
    "# Generate data using the CIFAR data generator\n",
    "vit_cifar_train_data = cifar_data_generator(cifar_x_train, cifar_y_train, batch_size=batch_size, shuffle=True)\n",
    "vit_cifar_val_data = cifar_data_generator(cifar_x_test, cifar_y_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train the ViT model on the CIFAR dataset\n",
    "start_time = time.time()\n",
    "vit_cifar_history = vit_cifar_model.fit(vit_cifar_train_data,\n",
    "                                       epochs=10,\n",
    "                                       steps_per_epoch=len(cifar_x_train) // 32 + 1,\n",
    "                                       validation_data=vit_cifar_val_data,\n",
    "                                       validation_steps=len(cifar_x_test) // 32 + 1,\n",
    "                                       callbacks=[cosine_annealing, warmup_lr])\n",
    "\n",
    "# Calculate the time taken for training\n",
    "vit_cifar_time = time.time() - start_time\n",
    "print(\"Time to fine-tune ViT on CIFAR dataset: {}\".format(round(vit_cifar_time, 4)))\n",
    "\n",
    "# Evaluate the performance on training and testing sets\n",
    "vit_cifar_train_result = vit_cifar_model.evaluate(cifar_x_train, cifar_y_train)\n",
    "vit_cifar_val_result = vit_cifar_model.evaluate(cifar_x_test, cifar_y_test)\n",
    "\n",
    "# Display the training and testing metrics\n",
    "print('')\n",
    "print('Training Accuracy: {}% and Training Loss: {}'.format(round(vit_cifar_train_result[1] * 100, 2), round(vit_cifar_train_result[0] * 100, 2)))\n",
    "print('Testing Accuracy: {}% and Testing Loss: {}'.format(round(vit_cifar_val_result[1] * 100, 2), round(vit_cifar_val_result[0]  * 100, 2)))\n",
    "print('')\n",
    "\n",
    "# Save the model data\n",
    "save_data(file_path, vit_cifar_model, vit_cifar_history, \"vit_cifar\")\n",
    "\n",
    "# Clear variables from memory\n",
    "del vit_cifar_model, vit_cifar_history, vit_cifar_train_result, vit_cifar_val_result"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rn_Voa-LLXA4",
    "DpRsu-Q-lrq8",
    "gFM9JIIk4qLz"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
